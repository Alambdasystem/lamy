import os
import mmap
import numpy as np
import multiprocessing as mp
import threading
import psutil
import time
from concurrent.futures import ProcessPoolExecutor
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer as HFTrainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset

# ---------------------------------------------------
# üöÄ NUMA Optimization for CPU Multi-Core Efficiency
# ---------------------------------------------------
def pin_process_to_cpu():
    """Pin process to first NUMA node (Windows & Linux)."""
    num_cpus = psutil.cpu_count(logical=False)
    cpu_list = [i for i in range(num_cpus // 2)]  # Use first half of CPUs

    try:
        p = psutil.Process()
        p.cpu_affinity(cpu_list)  # Set CPU affinity
        print(f"‚úÖ Process pinned to CPUs: {cpu_list}")
    except AttributeError:
        print("‚ö†Ô∏è CPU affinity setting is not supported on this OS.")

# ---------------------------------------------------
# üöÄ Precomputed Truth Tables (Supports Larger Values)
# ---------------------------------------------------
MAX_TRUTH = 4096  # ‚úÖ Increased to support larger values

# ‚úÖ FIX: Use `uint16` instead of `uint8` to prevent overflow
xor_table = np.fromfunction(lambda i, j: i ^ j, (MAX_TRUTH, MAX_TRUTH), dtype=np.uint16)
add_table = np.fromfunction(lambda i, j: (i + j) % MAX_TRUTH, (MAX_TRUTH, MAX_TRUTH), dtype=np.uint16)
mul_table = np.fromfunction(lambda i, j: (i * j) % MAX_TRUTH, (MAX_TRUTH, MAX_TRUTH), dtype=np.uint16)

# ---------------------------------------------------
# üöÄ Optimized SSD Memory Mapper for Model Weights
# ---------------------------------------------------
class SSDMapper:
    """Manages SSD-based memory mapping for large-scale weight storage."""
    
    def __init__(self, filename, total_params):
        self.filename = filename
        self.total_params = total_params
        self._init_weights_file()

        self.fd = os.open(filename, os.O_RDWR | os.O_DIRECT if os.name == "posix" else os.O_RDWR)
        self.mmap = mmap.mmap(self.fd, 0, access=mmap.ACCESS_WRITE)
        self.lock = threading.Lock()

    def _init_weights_file(self):
        """Ensure SSD weight file exists and has the correct number of elements."""
        if not os.path.exists(self.filename) or os.path.getsize(self.filename) != self.total_params * np.dtype(np.uint16).itemsize:
            print(f"‚ö†Ô∏è Reinitializing SSD weight file to {self.total_params} elements...")
            weights = np.random.randint(0, MAX_TRUTH, size=self.total_params, dtype=np.uint16)
            with open(self.filename, "wb") as f:
                f.write(weights.tobytes())

    def get_weights(self):
        """Retrieve mapped weights from SSD."""
        return np.frombuffer(self.mmap, dtype=np.uint16)

    def update_weights(self, new_weights, start_index=0):
        """Asynchronous batched SSD writes."""
        def async_write():
            with self.lock:
                np.copyto(np.frombuffer(self.mmap, dtype=np.uint16)[start_index:start_index + len(new_weights)], new_weights)
        thread = threading.Thread(target=async_write)
        thread.start()
        thread.join()

    def __del__(self):
        try:
            self.mmap.close()
        except BufferError:
            pass
        os.close(self.fd)

# ---------------------------------------------------
# üöÄ SSD-Backed Neural Network Training
# ---------------------------------------------------
class SSDTrainer:
    """Trainer that loads model weights from SSD and trains with minimal RAM usage."""

    def __init__(self, ssd_filename, total_params, vocab_size, embed_dim, mlp_hidden, output_dim):
        self.ssd = SSDMapper(ssd_filename, total_params)
        self.weights = self.ssd.get_weights()

        # Compute total needed parameters
        expected_params = vocab_size * embed_dim + embed_dim * mlp_hidden + mlp_hidden * output_dim
        if len(self.weights) < expected_params:
            raise ValueError(f"SSD weight file has {len(self.weights)} elements, but {expected_params} are required.")

        print(f"‚úÖ SSD weights loaded: {len(self.weights)} elements.")

        # Extract weight slices
        self.emb = self.weights[:vocab_size * embed_dim].reshape(vocab_size, embed_dim)
        mlp_start = vocab_size * embed_dim
        mlp_end = mlp_start + embed_dim * mlp_hidden
        self.mlp = self.weights[mlp_start:mlp_end].reshape(embed_dim, mlp_hidden)
        output_start = mlp_end
        output_end = output_start + mlp_hidden * output_dim
        self.output = self.weights[output_start:output_end].reshape(mlp_hidden, output_dim)

        print("‚úÖ Model weights initialized successfully.")

    def train(self, steps=50000):
        """Minimize memory by loading only needed data."""
        print("üöÄ Training started with SSD-mapped memory...")
        for step in range(steps):
            input_ids = np.random.randint(0, self.emb.shape[0], size=(64,))
            hidden = xor_table[input_ids, self.mlp.sum() % MAX_TRUTH]  # Fixed shape mismatch
            logits = add_table[hidden, self.output.sum() % MAX_TRUTH]  # Fixed shape mismatch

            # ‚úÖ SSD batched weight updates
            self.ssd.update_weights(logits, start_index=0)  # Update the correct portion of the weights

            if step % 1000 == 0:
                print(f"‚úÖ Step {step} completed.")
        print("üéâ Training complete!")

# ---------------------------------------------------
# üöÄ Hugging Face Fine-Tuning with SSD Mapping
# ---------------------------------------------------
training_args = TrainingArguments(
    per_device_train_batch_size=8,  # üî• Lowered for low RAM usage
    gradient_accumulation_steps=1,  # üî• Minimized for efficiency
    num_train_epochs=1,  # üî• Faster training
    optim="adamw_torch_fused",
    logging_steps=1,  # ‚úÖ Log every step
)

def train_model():
    """Function to train the model in a separate thread."""
    trainer = SSDTrainer("optimized_weights.bin", 50000, 100, 64, 256, 100)
    trainer.train(steps=50000)

def reload_model_periodically():
    """Function to reload the model every minute."""
    global hf_trainer
    while True:
        time.sleep(60)
        hf_trainer = HFTrainer(
            model=GPT2LMHeadModel.from_pretrained(model_name),
            args=training_args,
            train_dataset=tokenized_datasets,
            data_collator=data_collator,
        )
        print("‚úÖ Model reloaded.")

if __name__ == "__main__":
    mp.freeze_support()  # ‚úÖ Windows multiprocessing fix

    model_name = "gpt2"
    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # ‚úÖ Generate Training Data
    nouns = ["dog", "cat", "mouse", "bird", "child", "teacher", "student"]
    verbs = ["eats", "chases", "sees", "likes", "hugs", "teaches", "learns"]

    def generate_sentence():
        return f"{np.random.choice(nouns)} {np.random.choice(verbs)} {np.random.choice(nouns)}."

    num_sentences = 50000
    training_sentences = [generate_sentence() for _ in range(num_sentences)]
    print(f"Generated {len(training_sentences)} valid training sentences.")

    data = {"text": training_sentences}
    dataset = Dataset.from_dict(data)

    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=32)

    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=1  # ‚úÖ Avoid Windows multiprocessing issues
    )

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # Initialize the global hf_trainer variable
    hf_trainer = HFTrainer(
        model=GPT2LMHeadModel.from_pretrained(model_name),
        args=training_args,
        train_dataset=tokenized_datasets,
        data_collator=data_collator,
    )

    # Start training in a separate thread
    training_thread = threading.Thread(target=train_model)
    training_thread.start()

    # Start reloading the model periodically in a separate thread
    reloading_thread = threading.Thread(target=reload_model_periodically)
    reloading_thread.start()

    # Handle user input in the main thread
    while True:
        user_input = input("You: ")
        if user_input.lower() == "quit":
            break
        inputs = tokenizer(user_input, return_tensors="pt")
        if inputs["input_ids"].numel() == 0:
            print("Invalid input. Please enter a valid sentence.")
            continue
        outputs = hf_trainer.model.generate(**inputs, max_length=50)
        print(f"AI: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
